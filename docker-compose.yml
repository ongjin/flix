version: "3"
services:
    zookeeper:
        image: confluentinc/cp-zookeeper:latest
        container_name: zookeeper
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000
        ports:
            - "2181:2181"
        networks:
            - msanetwork

    kafka:
        image: confluentinc/cp-kafka:latest
        container_name: kafka
        depends_on:
            - zookeeper
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
            # KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9092
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
            KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        ports:
            - "9092:9092"
            - "29092:29092"
        networks:
            - msanetwork
        healthcheck:
            test:
                [
                    "CMD",
                    "kafka-topics",
                    "--list",
                    "--bootstrap-server",
                    "localhost:9092",
                ]
            interval: 10s
            timeout: 5s
            retries: 10

    redis:
        image: redis:latest
        container_name: redis
        ports:
            - "6379:6379"
        networks:
            - msanetwork

    postgres:
        image: postgres:latest
        container_name: postgres
        restart: always
        environment:
            # POSTGRES_DB: flix_auth
            # POSTGRES_DB: flix_session
            # POSTGRES_DB: flix_streaming
            POSTGRES_DB: flix
            POSTGRES_USER: dydwls140 # DB 사용자명
            POSTGRES_PASSWORD: "@astems1027" # DB 비밀번호
        ports:
            - "5432:5432"
        volumes:
            - postgres_data:/var/lib/postgresql/data # 데이터 영구 저장
            # - ./postgres-init:/docker-entrypoint-initdb.d # 초기 SQL 실행
        networks:
            - msanetwork

    elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:7.10.0
        container_name: elasticsearch
        environment:
            - discovery.type=single-node
            - ES_JAVA_OPTS=-Xms1g -Xmx1g
            - bootstrap.memory_lock=true # 메모리 잠금 활성화 (호스트에서 memlock 설정 필요)
        ulimits:
            memlock:
                soft: -1
                hard: -1
        ports:
            - "9200:9200"
            - "9300:9300"
        volumes:
            - esdata:/usr/share/elasticsearch/data
        networks:
            - msanetwork

    logstash:
        image: docker.elastic.co/logstash/logstash:7.10.0
        container_name: logstash
        depends_on:
            - elasticsearch
        ports:
            - "5044:5044" # Beats input 등 사용 시
        volumes:
            # Logstash 설정 파일 및 파이프라인 정의 파일을 로컬에서 매핑합니다.
            - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
            - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
        networks:
            - msanetwork

    kibana:
        image: docker.elastic.co/kibana/kibana:7.10.0
        container_name: kibana
        depends_on:
            - elasticsearch
        ports:
            - "5601:5601"
        networks:
            - msanetwork

    filebeat:
        image: docker.elastic.co/beats/filebeat:7.10.0
        container_name: filebeat
        user: root
        volumes:
            - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
            - ./logs:/logs:ro
        networks:
            - msanetwork
        depends_on:
            kafka:
                condition: service_healthy
        command: ["filebeat", "-e"]

volumes:
    postgres_data:
    esdata:

networks:
    msanetwork:
        driver: bridge
# docker-compose up

# docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic session-topic --from-beginning
# docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic streaming --from-beginning
# docker exec -it kafka kafka-console-consumer --bootstrap-server kafka:9092 --topic spring-logs --from-beginning
# docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning

# redis-cli -h localhost -p 6379
# get
